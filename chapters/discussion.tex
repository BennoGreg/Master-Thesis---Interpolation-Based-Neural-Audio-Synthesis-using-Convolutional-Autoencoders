\chapter{Evaluation}
\label{cha:Discussion}

In this chapter the results (see chapter \ref{cha:Results}) that could be obtained by the experiments shown in chapter \ref{cha:Experiment} are discussed. As the results of the experiments cannot solely be described through graphics and numbers shown in the previous chapter, the evaluation also takes the auditory results into account. Based on the combination of the graphical and auditory results, some interesting findings and observations can be derived that get discussed in the following sections.

%\section{Interpretation of results / observations}

\section{Impact of model configurations}
\label{sec:disc_model_conf}
Throughout the experiments different models have been trained and utilized for reconstruction or synthesis of audio spectrograms. With these experiments, the goal was to find a model that is suited for the task of (re)synthesizing audio data. Furthermore by exploring different models with different configurations, insight could be gained, on how these affect the performance but also the output of the model. In the case of this work, the results of the experiments can be presented in different ways. In the previous chapter \ref{cha:Results}, the results have been depicted as error scores, but also the embedded space data and the resulting spectrograms. With those alone, the approaches cannot be fully evaluated, as it is also important to consider and discuss the auditory quality, as it dealt with audio data. The following section therefore takes the numerical and graphical but also auditory results into account to evaluate the models with regard to their configuration. In total the evaluation takes the results of 7 different models into account whereas 3 of them get discussed later on in section \ref{sec:disc_imp_pre_processing}.

\subsection{Impact on score}
\label{subsec:disc_conf_score}
To train a neural network, but also other machine learning models, an error score is needed. With the help of this score, the network gets optimized to minimize the error and thus improve the networks performance and quality of output. In chapter \ref{cha:Results} the error scores regarding each network configuration are displayed separately regarding the network type (1D or 2D convolutional) and input type (log-mel vs. log-magnitude spectrograms).

By comparing the error scores regarding the network type it can be said, that the 1D convolutional network performed best, as it shows the lowest scores on training and validation, but especially testing. All networks that use 2D convolutions and operate on snippets coming either from log-magnitude or log-mel spectrograms have a worse score. To mention those scores just show the amount of error between the reconstructed and the input data. Furthermore when also considering the amount of strides beeing used an interesting observation can be found regarding the score. Here it can be said, that the more striding is applied in the network, the higher gets the error. This was observed using the 2D convolutional networks, as with the 1D convolutional networks no strides have been applied. Mentioning also the scores concerning specific pitch classes, it can be said that the 1D convolutional network has the best scores in all pitch classes, having equal scores amongst them. Moving on to the 2D convolutional networks, the scores of the single pitch classes are also higher than the ones from the 1D convolutional network. Also they increase with the amount of striding. Nevertheless it can be seen that concerning the 2D convolutional networks with log-magnitude spectrograms and 1D convolutional networks, they all show constant scores alongside the pitch classes. An exception are the three networks trained on log-mel spectrograms as those increase with the pitch class. As this is mainly influenced by the type of input, this gets discussed later on in section \ref{sec:disc_imp_pre_processing}. 

In total it can be said that 1D convolutional networks yield a better score than 2D convolutional networks having also the best scores among specific pitch classes. Furthermore regarding the 2D convolutional networks it can be said that the amount of striding certainly influences the models score, as the more striding is applied, the higher the error gets in the network. Nevertheless just of this score, it cannot be concluded that the output sound using a 1D convolutional network is also of better quality. This can also be said concerning the stridings within the network, as the output has to be evaluated further on by listing to the final sounds. Furthermore as the error scores for the specific pitch classes do not increase (models with log-magnitude spectrograms), it also cannot be said, that the output of all pitch classes is equally good in its auditory quality. Therefore in the next section \ref{subsec:disc_conf_single_rec} the outputs being spectrogams, embeddings but also the generated audible signals, get discussed in order to evaluate the impact of the models configuration properly.

\subsection{Impact on single reconstruction}
\label{subsec:disc_conf_single_rec}
As in the previous subsection \ref{subsec:disc_conf_score} the impact of model configuration on the score has been discussed, here the generated experimental artefacts such as spectrograms, embeddings and audios get included into the evaluation. Like in the chapter \ref{cha:Results} the discussion here gets divided into two parts, whereas in this part the impact on the single reconstruction and in the following part (subsection \ref{subsec:disc_conf_inter}) the impact on the interpolated (synthesized) outcome gets evaluated. 
This subsection also mainly deals with the outcome of the networks using log-magnitude data. The discussion regarding the log-mel spectrograms will follow later on in section \ref{sec:disc_imp_pre_processing}.


\subsubsection{Impact on generated spectrograms and audio}
In this section the artefacts like spectrograms and audios gained by reconstructing single audios without altering the embeddings get assessed. This should bring insights into how the model configuration affects the ability to reconstruct audio spectrograms but also the quality of the output. As an instrument which represents the results, the guitar sample shown in the result chapter is chosen. This guitar sample contains characteristics such as the guitar stroke but also the damping of the strings at the end of the sample. Also it has the characteristic that it fades over time. To take the spectrograms and embeddings shown in chapter \ref{cha:Results} some first observations can be made. First on by comparing the output spectrograms of the 1D convolutional network, and those of the 2D convolutional network, it can be seen that the original areas containing little energy (dark) get better preserved in the 2D convolutional networks. The output of the 1D convolutional network appears to have more energy distributed in areas of little energy. Further on it also can be observed that especially the single stride 2D convolutional network is able to preserve the broadband spectra at the beginning and the end of the spectrograms. In comparison the 1D convolutional network has difficulties in preserving those broadband spectra. This can be also justified when listening to the final audio. As a note, unless it is mentioned, the evaluation on the audible output is done on samples generated with the original phase information. Concerning the output of the 1D convolutional network the stroke and damping is not present but it is in the ones coming from the 2D convolutional networks. Also the sample of the 1D convolutional network has a worse listening quality as it gets louder from the beginning on and becomes noisy. The latter especially gets proved by the fact that the spectrogram contains more energy across all frequencies that have usually little energy contained. Further on by listening to the outputs of the 2D convolutional network, those are overall better in their audible quality but also when looking onto the spectrograms. Especially concerning the single-stride network its output has the best quality, as it preserves the important characteristics such as stroke and the damp best, but also fades like the original sample. By comparing especially this one, hardly no difference can be noticed, except a small lack in the intensity of the stroke. When listening to the outputs of the double- and triple-strided network, no significant differences can be noticed, as also characteristics, like stroke, fade and damping are still contained. Just the output of the triple-stride network is slightly distorted, being not as clear as the output of the others, when listening closely. Taking also its corresponding spectrogram into account, those distortions can be justified by the less precise harmonics. Similar observations can be also made by using a sample of a different instrument (e.g. brass). Despite this one not fading over time and does not have characteristics like a guitar (stroke, damping), the output is also of similiar quality like with the guitar. Also the same conclusions can be made, as the quality using the 2D convolutional network is better than with the 1D convolutional network, having equally good sounds using single- and double-striding. 

As those samples were not altered in their embedding, the original phase information could be reused for the inverse STFT to obtain the sounds. Those have been taken into account in the last paragraph. For the comparison but also as for synthesized spectrograms no phase information is present, samples have been generated using the Griffin Lim algorithm \cite{Griffin1984} to estimate the signal. Using the same guitar samples that have been used for discussion before but generated with the Griffin Lim algorithm introduced some differences in the sound quality especially concerning the 2D convolutional network. Taking again the output of the 1D convolutional network into account, by listening to the signal, the quality does not change. Interestingly while listening to the samples generated by the 2D convolutional networks, drawbacks between the ones using Griffin Lim and the others with the original phase information, can be observed. As the ones with the preserved phase fade clearly towards the end, the ones being estimated by Griffin Lim, alternate slightly in their loudness.

Previously in subsection \ref{subsec:disc_conf_score} the scores regarding their pitch class have been discussed. As those cannot solely be considered for grading the performance and output quality, the audible outputs have to be taken into account. As this cannot be done for all samples among all instruments, guitar acoustic samples were picked for evaluation. When listening to the generated samples of the 1D convolutional network, it can be noticed, that among the pitch classes (going from 030 to 100) all the samples have the characteristic in becoming louder towards the end. Furthermore also especially when the pitch gets higher, that the samples contain more distortion. If the final sound gets reconstructed with the Griffin Lim algorithm, an interesting behaviour can be observed. With special regard to the lower pitched samples e.g. midi note 030, the pitch is not recognizable in the output. Later on when using samples of higher pitch classes, the pitch becomes recognizable. 
Coming to the samples generated by the 2D convolutional network, here the output samples of the single- and double-stride networks remain the same in its quality alongside the pitch (original phase). Also no difference between the two networks can be perceived. Regarding the triple-stride network, more distortion can be perceived in the lower pitched samples, while when ascending in the pitch, the distortion gets less and less perceptible. Again when taking the results obtained by applying the Griffin Lim algorithm, the pitch cannot be recognized in the output samples using lower pitched samples. Especially when applying more striding like in the triple-stride network, this behaviour appears more significantly. Having the original phase information this would not be an issue, but in the case of synthesis where the output spectrogram gets reconstructed from an altered embedding and no phase information is present, it is. 

\subsubsection{Impact on generated embeddings}
Finally after discussing the impact of the model configuration on the spectrograms but especially outputs, it is also of interest, how the embeddings differ from model to model. Especially as those represent the features from which the decoder part reconstructs a spectrogram. In chapter \ref{cha:Results} the embeddings of a guitar sample have been depicted together with their corresponding recreated spectrograms. First of all it can be seen, that all embeddings in general are a projection of the input spectrograms on a lower dimensional space. Different to the embeddings generated by the 2D convolutional networks, the ones from the 1D convolutional network preserve the high energy areas with negative values and vice versa. As also in the output spectrograms and samples of the 2D network, the broadband spectra are preserved better, those can be also seen in the embeddings at the beggining and the end. This happens in contrast to the 1D convolutional network, as like in the output, the broadband spectra are not recognizable. With a final look onto the spectrograms generated by the 2D convolutional networks, it can be seen, that the more striding is applied the less values are available to preserve the inputs features. With regard to the triple-stride network, the embedding contains the essential features of the input, but regarding its output it lacks to preserve the desired quality which was discussed before. Therefore in this case, the embedding contains little information to reconstruct a spectrogram with a desired quality. This also makes the single- and double-stride network favorable as they preserve enough data to reconstruct spectrograms with a sufficient quality.


\subsection{Impact on interpolation}
\label{subsec:disc_conf_inter}
In subsection \ref{subsec:disc_conf_single_rec} the developed models got evaluated towards their ability to reconstruct single audio spectrograms. There already promising results and interesting findings could be obtained that are important for synthesizing audio. Again like previously in this section the 1D convolutional but also the single-, double- and triple-stride 2D convolutional networks using log-magnitude spectrograms get taken into account. The evaluation for synthesizing sounds with networks taking log-mel spectrograms as input will be also be done later on in section \ref{sec:disc_imp_pre_processing}. Discussed in this section will be the artifacts like embeddings, output spectrograms, but also output sounds. Furthermore during the experiments correlation coefficients between the embeddings were calculated to obtain interesting combinations of embeddings that differ in their characteristics. This was not done with the embeddings of the 1D convolutional network. Nevertheless interesting combinations for the audio synthesis could be found. For the purpose of these experiments, two instruments (guitar acoustic and brass acoustic) were selected that have significant differences and should synthesize an interesting sound. 

\subsubsection{Observations regarding correlation of embeddings}
Regarding their correlation coefficients those have been 0,16 (single-stride), 0,0052 (double-stride) and -0,17 (triple-stride). By comparing those values to other combinations, those signalize a low correlation and thus being highly different. As a note it could also be examined that some examples being of the the same instrument family have a high correlation which means they are similar and contain similar characteristics. With this knowledge it can also be approved that the networks learn to capture features that describe the instrumental source. Some correlation matrices can be seen in the supplementary material to the thesis as they cannot be depicted sufficiently on paper. Comparing the correlation matrices over those three 2D convolutional networks no significant differences can be observed.

\subsubsection{Impact on generated artefacts}
When taking a look on the results in chapter \ref{cha:Results} besides the embeddings and spectrograms concerning single reconstructions also the results of synthesized spectrograms are shown. There the generated embeddings of two instruments were shown in combination with the resulting interpolated embedding. Alongside those, the generated output spectrograms are shown, that represent the synthesized audio. Like mentioned before, as source samples a guitar and brass sample has been chosen to serve as base for the synthesis task. First taking the results using the 1D convolutional network into account, there it could be seen that by interpolating the embeddings the resulting new embedding contains the features of both instruments. Nevertheless by comparing it to both input spectrograms it can be seen, that especially the guitar features cannot be recognized that easily. This behaviour can also be recognized by looking onto the resulting spectrograms and comparing it to the two input spectrograms. Further on by taking the results using the 2D convolutional networks into account, improvements can be detected. First of all when looking onto the interpolated embeddings, here the instruments features can be distinguished better, especially the guitar sample can be recognized better. This behaviour also varies having different amounts of striding in the networks. In particular when examining the interpolated embeddings of the double- and triple-stride network the features of both instruments can be visually distinguished the best. 

When moving on onto the spectrograms, the best quality in terms of preserving the fine structures of the source instruments delivers the single-stride network. Also contrary to the output spectrogram of the 1D convolutional network both instruments can be recognized better. The output spectrogram of the double stride network appears to preserve the distinct sounds even better, but lacks in preserving the fine structures and changes of the input spectrograms. With a final look onto the output of the triple-stride network, this one has hardly any fine structures preserved as the the harmonic features over time also appear "washed out" and being not recognizable precisely. This can be brought into relation with the contained features in the embedding, as less features are preserved.

To also bring the quality of the audible output into relation some further interesting findings can be obtained. Like also with the single reconstruction experiments, the output of the 1D convolutional network does not contain a guitar stroke, and also proves that the guitar is less audible then the brass sample. Furthermore it is also in its amplitude louder then the original samples and contains distortion which is certainly not desired. In comparison the outputs generated by the 2D convolutional networks have an overall better perceptual quality. Interestingly as the single- and double-stride networks' output have differences in the spectrograms, those cannot be perceived in the final output sound making them sound equally. The guitar also can be perceived better with the sound characteristics of the brass sample. Furthermore also the stroke is preserved in the output. As a further advantage to the output of the 1D convolutional network also no distortion is contained, making it sound clearly. Finally listening to the output of the triple-stride network it can be said that the quality is again significantly worse then with the single- and double-stride networks. Nevertheless the instruments are both perceptable and distinguishable in the output, but it appears to have missing frequency parts, making it sound more "metallic". The latter can also be brought into relation with the "washed out" harmonics in the spectrogram. Furthermore also the pitch is unstable as it rises slightly in the end of the sample. 


\section{Impact of pre-processing}
\label{sec:disc_imp_pre_processing}
Having discussed the impact of the model configuration and composition, in this section the impact the pre-processing on this kind of machine learning problem has, gets discussed. Like it has been described in chapter \ref{cha:Approach}, section \ref{sec:app_pre-processing}, the step of pre-processing is an important component of every machine-learning task. Especially as it also has a significant influence on the performance of the machine-learning model. This means also, that it has a significant influence on the output and its quality, with special regard to the outcome of this work. Therefore this section describes the influence of pre-processing that has been observed during this work. 

For the task of neural audio synthesis, there exist different methods that differ mainly in the shape of their input data. While there exist methods that use the time-domain signal, a few methods including this one, use spectrograms for this task. To generate spectrograms from audio, the pre-processing stage incorporates this task. 

\subsection{Configuration of STFT}
Like discussed earlier, to obtain spectrograms, the STFT was applied which could be parameterized. This parameterization includes the size of the signal window, on which the Fourier transform gets applied to. Throughout this work this parameter got altered, as it influences the frequency- but also time-resolution of the spectrogram. First on this parameter was chosen to be 512 which means a frequency resolution of 31,25 Hz and time 32 ms (with sampling rate of 16 kHz). Due to the unsatisfactory model performance, this value has been changed to 1024 to have a frequency resolution of 15,625 Hz and time-resolution of 64 ms. This also has the advantage of having a better frequency resolution and therefore also more features to learn from along the frequency axis. Therefore this value was chosen throughout all experiments. When wanting to develop a real-time capable system the time resolution has to be taken into account, as having a better frequency resolution the resulting time frames increase. This means that the duration of the audio that has to be available for the model increases. Therefore it has to be said that the right configuration needs to be found to obtain the right frequency-time trade-off that is best for the problem.

\subsection{Use of single frequency vectors vs. spectrogram frames}
As it could be seen throughout the experiments, having the spectrograms, those can also be differently prepared and used for the model. First on just the single frequency vectors have been chosen to be used as input for the model. Later on overlapping frames of spectrogram slices were utilized as model inputs. Using the first one, this has the advantage that just one frequency vector is needed as input to generate an output. Especially in terms of aiming to develop a system that performs real-time audio synthesis this would be the ideal choice. Nevertheless by comparing the results of those two approaches, the latter is proven to have a better output quality. The details to this discussion and results have already been discussed in section \ref{sec:disc_model_conf}, as this also deals with the model configuration. 

\subsection{Magnitude vs. Mel}
As mentionend before, there are also other methods on how to use audio data for machine learnig tasks. Except from using the raw time-domain signal, there also exist different types of spectrograms that can be used. First on during this work log-magnitude spectrograms have been used as the source input data for the models. Further on in the final experiments log-mel spectrograms have been used, which are based on the mel scale. By perfoming the same experiments as with log-magnitude spectrograms, the results should bring an insight on the impact, the pre-processing or use of a different scale has. Again three differently configured networks have been implemented and considered for the experiments. Those again differ in the amount of striding (single, double, triple) on each network side. In the last chapter the error scores, but also the output spectrograms and embeddings of the models, using both types of spectrograms have been shown. With those results it already can be seen that it has a significant impact on how the audio data is pre-processed and provided for the neural network. 

\subsubsection{Impact on score}
First of all regarding the error scores it can be said that, without taking the networks configuration into account, better scores could be reached, using the log-magnitude spectrograms. The scores between the single-, double- and triple-stride networks with log-mel spectrograms also rise (more significantly) the more striding is applied. Regarding the scores for single pitch classes, different behaviour could be observed. As the scores in the log-magnitude spectrogram trained networks remain equal among the pitch classes, the ones using log-mel spectrogram increase with the pitch. This means that using the log-mel spectrograms, the networks have a bigger error and thus more difficulties in reconstructing higher pitched samples than lower ones. On the other side the networks with log-magnitude spectrograms can reconstruct the input equally well, with no regard to the pitch (concerning the scores). As this is just the performance of the network, regarding the reconstruction error, this does not automatically mean, that the listening quality is better or worse. Thus again the artefacts like embeddings, spectrograms but also audio samples have to be taken into the evaluation part which gets done in the following subsection. The observations regarding log-magnitude spectral data has already been discussed before in subsection \ref{subsec:disc_conf_single_rec}.

\subsubsection{Impact on single reconstruction}
Again like with log-magnitude spectrograms, experiments have been conducted in using log-mel spectrograms to be reconstructed. The results in chapter \ref{cha:Results} show again the reconstructed spectrograms with their corresponding embeddings among the three different networks. For comparative reasons, again the same instrument source have been elected for these experiments. Therefore key differences between applying log-mel and log-magnitude should be discovered. Back in the previous chapter, it already has been said that the output spectrograms are lacking in preserving the higher harmonics. This problem also increases the more striding is applied. Furthermore with the triple-stride network, some artifacts in the upper harmonics could be perceived in the output spectrogram. Having less precise upper harmonics can be also brought into relation with the nature of the mel scale, as lower frequencies are more emphasized due to the larger distance on the scale as the higher frequencies. Despite of that as a guitar sample gets examined, the broadband spectra at the beginning and end are also present. Compared to the log-magnitude embeddings, the here generated embeddings again value the original high-energy areas with negative numbers and vice versa. Nevertheless the lack of precision in the higher harmonics can also be seen in those embeddings, as with increasing the stride, less values can be captured for the original dense higher harmonics. Despite of this the lower harmonics are still visible as negatively valued areas. 

By also listening to the corresponding output samples, some final findings can be formulated. First of all if not explicitly mentioned, the original phase information was reused concerning the discussed output samples. To the overall quality it can be said, that it is less good as with log-magnitude spectrograms, as they do not have a clear sound (alternating loudness). Nevertheless features like stroke and damping of the strings can be perceived, as well as the fading of the sound. The output of the double-stride network sounds less clear, having more alternations in loudness then the one coming from the single-stride network. The output of the triple-stride network again is of the lowest quality, as it contains audible artefacts at the beginning of the sound, which are also visible in the output spectrogram. Listening to the same sounds that were regenerated with the Griffin Lim algorithm minor drawbacks in quality can be perceived. While the one coming from the single-stride network introduces more alternation in the loudness, the outputs of the double- and triple-stride networks are unstable in their pitch. 

Bringing the pitch into account, the scores for the pitch classes alongside the three different strided networks, have been examined. Like with the log-magnitude networks also the audible output samples have to be considered. Again the acoustic guitar has been chosen, from which samples along the pitch classes were picked to evaluate the quality. As it has been discussed, the scores increased while ascending in the pitch class which should mean that higher pitched samples are of a lower reconstruction quality. By listening to samples of all three different strided networks, no difference can be observed in the quality regarding the pitch. Taking again the output obtained with Griffin Lim algorithm into account, similar observations can be made like with log-magnitude spectrograms. Especially with the double- and triple-stride networks, the effect of having no recognizable pitch information present regarding lower pitch-classes. Ascending in the pitch class the pitch is present, but mostly remaining unstable or slightly alternating over time. The single-stride network again performs best. Despite of the lack of pitch in the lower classes, the overall quality remains equal.

\subsubsection{Impact on interpolation}
As in the previous section the results of reconstructing single audios using log-mel spectrograms, this point evaluates the capability of synthesizing audio based on interpolating embeddings. This has already been done using log-magnitude spectrograms in subsection \ref{subsec:disc_conf_inter}. The evaluation for log-mel spectrograms will be done in the same manner, as the results depicted in chapter \ref{cha:Results} will be taken into account in combination with assessing the audible quality. By this the impact of using log-mel spectrograms compared to log-magnitude gets discussed. Again the same two instrument sources were used (guitar acoustic and brass acoustic) to synthesize audio.

During the experiment, like in previous ones, correlation matrices were calculated to obtain an overview, of which samples are least similar and thus form an interesting combination. For this combination the correlation coefficients have been 0,31 (single-stride), 0,62 (double-stride) and 0,53 (triple-stride) which are significantly higher then those regarding the log-magnitude networks. Also when comparing the correlation matrices of the experiments in \ref{subsec:disc_conf_inter} with those generated with the embeddings of the log-mel networks it can be seen, that those values are significantly different. Also within the log-mel networks the correlations matrices and their values differ a lot. This means that while in one network two instruments have a low correlation, in another one those have a high correlation. 
For example as seen with the values mentioned before it can be said that the embeddings generated of log-magnitude spectrograms have more distinct features then those generated of log-mel spectrograms. Thus it can be said that those networks extract the features differently.

Coming to the results of the experiments that were obtained by synthesizing audio using interpolation in the embedded space. By comparing all output spectrograms that are shown in the previous chapter, the one generated by the single-stride network,preserved the input features the best. Especially because the features of both instruments can be recognized sufficiently in the output. With special regard to the characteristics of the brass samples, those are preserved best, as the fine harmonic structures still can be seen in the output. The output spectrogram of the double-stride network already lacks in preserving most of the harmonical features of the upper frequency range of the brass sample. Nevertheless the low frequency harmonics are still preserved in a good manner.
This is also mostly because the features representing the energies of the lower frequencies, are similar regarding both instruments. 
Furthermore the features representing the original high-frequency areas, are also less contained in the embeddings and thus those cannot be sufficiently reconstructed in the output. With a final look onto the output of the triple-stride network, this one also does not contain the harmonics in the upper frequencies and also introduces features that are not contained in the original input spectrograms. By listening to the output samples, those findings can also be confirmed. There also the output of the single-stride network has the best quality, as it preserves both instruments and makes them perceivable. Just like in the output spectrogram the resulting sound of the double-stride network misses the higher harmonical features of the brass. Finally when listening to the output of the triple stride network, this one contains audible artefacts like harmonics that are not present in the input samples but can be seen in the output spectrogram. Also the high harmonical features are not present anymore.

Making the fact, that in terms of using log-mel spectrograms, the single-stride network generates audios with the best quality while more striding introduces undesired artefacts but also misses features especially in higher harmonical ranges. Generally the quality compared to the approaches using log-magnitude spectrograms and 2D convolutional networks is lower. 

\subsection{Spectrograms vs. Time-domain}
Knowing now that the pre-processing has a significant impact on the model performance but also quality of the audible output, it can be also discussed if spectrograms are suited best for this task. Another form of input except of spectrograms would be e.g. using time-domain signals. One has been discussed in the work of \textit{Engel et al.} \cite{Engel2017} where time-domain signal was used as input for a WaveNet-style autoencoder network. In their work they proved the WaveNet-style autoencoder approach to be advantageous, especially regarding the audible quality of the output. Having discussed the impacts the generation of spectrograms has on the task of audio synthesis, it can be said, that a few limitations showed up. As discussed before, the configuration of the STFT has an impact on the frequency resolution but also on the time resolution. For example for going towards real-time applications a high time resolution is desireable, as shorter time-frames can be taken as input for the model. This has the disadvantage of having a bad frequency-resolution which means to have less information present per time-frame. Furthermore as especially with this approach convolutional networks are applied, the phase information cannot be used. Leading to the fact of having magnitude data as output, to which than the phase has to be estimated at the end to regenerate the audible signal, as it is missing (regarding interpolated output). Like evaluated before this also affects the quality as the output regenerated with the original phase information is of higher quality than the estimated one. Knowing these problems, those would not be present, when using the time-domain signal as input. Also no calculations regarding transformations have to be made, which also speeds up the runtime. By taking the time-domain signal, this would request a different model type. Taking the by \textit{Engel et al.} proposed WaveNet-style autoencoder, this one operates on raw time-domain signal but has a more complex structure then convolutional autoencoders.

\section{Impact of post-processing}
Having now discussed the findings and observations by applying different network configurations but also using different pre-processing settings, this section deals with the effect of additional post-processing steps. With regard to the already discussed results, those were not using any additional steps. As discussed, especially with the 1D convolutional network, certain features of like the stroke or damping of guitar strings do not get preserved in the output sufficiently. By introducing 2D convolutional networks that use overlapping log-magnitude and log-mel specgtrograms, these issues could be improved significantly. 

\subsection{Energy correction}
Nevertheless to further improve such features but also to better approximate the original amplitude, additional steps were introduced that have been mentioned in chapter \ref{cha:Experiment} under subsection \ref{subsec:exp_rec_post}. To mention again briefly, the sum of energy values across each frequency vector of the input spectrogram has been computed in advance. The same has been done on the output spectrogram, to compare it to the original energy values and calculate a coefficient to adjust the energy in the output. This technique certainly improved the sound of the output, especially if single audios were reconstructed. With special regards to the guitar samples, this improved the presence of the guitar stroke or the damping. But also when listening to other reconstructed samples, like brass, it can be observed, that also the loudness is better aligned with the input.

When applying this post-processing step with the interpolated outcome, some further interesting findings can be formulated. Especially as there are two input spectrograms the average energy between the two input spectrograms, respective their frequency vectors, is taken for the correction. For example taking the synthesized sounds that have been discussed before, those were created without additional post-processing. By applying the energy correction, it can be observed that especially the loudness is affected by the correction. Without it still contains the fading of the guitar, whereas by equalizing the energy, the samples rather keep the same loudness or even increase in loudness. Nevertheless the guitar stroke is better perceivable. This concerns the approaches using log-magnitude but also log-mel spectrograms. 

\section{Differences between instruments}
No experiments were done that observed differences that could observed regarding the instrumental source. Nevertheless, when reconstructing single samples, it could be observed that some instruments classes or even samples have better scores then others. Especially in the initial experiments, where originally just two instrument sources were considered for training. There it could be observed, that e.g. acoustic guitar samples led to a significantly worse score in training than synthetic keyboard samples. But also after training with the whole dataset, it could be observed, that some classes performed worse regarding their test score. As this has not been a main research question during the recent work, this would be subject of further studies. 

\section{Comparison to other approaches}
In the chapter \ref{cha:related_works} a few approaches regarding neural audio synthesis have been described, which also served as a base for this research. Especially mentioning those that implemented autoencoder networks to synthesize audio. Mentioning those, some of the approaches used the output of the encoder parts to interpolate them in order to generate novel sounds. This approach was also applied in this work while some approaches like those coming from \textit{Colonel et al.} synthesized audio by omitting the encoder part and directly activating the embedded space. Most of the related work, that are specifically referenced as approaches with neural audio synthesis, also implemented other approaches that use different types of neural networks but in the shape of autoencoders. This approach solely emphasized on convolutional neural networks designed as autoencoders, that work with audio data shaped as spectrograms. \textit{Engel et al.} compared the use of a convolutional autoencoder to a WaveNet-style autoencoder, where the first one acted as a baseline model. Despite proving the WaveNet-style encoder as qualitatively better approach, the spectral autoencoder also yielded promising results. This can also be said for the outcome of this work, as this work also generated promising results and interesting findings.

Similar to other works, this work also implemented different networks to assess which type or which configuration can be applied best for the task of audio synthesis. These configurations differ mainly in the type of convolutions (1D or 2D), the amount of layers but also configuration of striding, while the basic composition of layers (1D/2D convolution - (leaky) ReLU - Batch Normalization) stayed the same. This happened contrary to the other approaches, as those mainly did research on different compositions not only incorporating convolutions. Furthermore this work solely focused on the work on pure log-magnitude and log-mel spectrograms as input data, without additional input augmention. Also no additional conditioning of the embedding (e.g. for the pitch) has been done in this work. Nevertheless, compelling and interesting results could be obtained, that prove the ability of convolutional autoencoders with spectrograms to synthesize sounds based on interpolation in the embedded space. Furthermore this work gives a special insight on how different parameterized convolutional autoencoders influence the quality of the output with regard to neural audio synthesis.



\section{Limitations and difficulties}
During the course of this work, some difficulties but also limitations occurred that had an impact on the work in progress but also on the outcome. First of all one of the major difficulties, especially in the beginning, was the ability to properly train a (complex) neural network, as high computational power is needed, which was not present at the beginning. Nevertheless this difficulty could be overcome, as for the purpose of this project, access to a GPU-accelerated computing service could be provided. This one improved and sped up the training process which made it possible to perform trainings on a large scale.

Another difficulty was how to properly display and the outcome, as this work deals with audio data. As it could be seen this could be solved by displaying the results as spectrograms but also with error scores. Nevertheless a full evaluation based on those artefacts could not be made, as it doesn't have a meaningfulness about the sound quality. For this case the outcome also had to be evaluated by listening to the resulting sounds to make a grading about the sound quality. Furthermore in case of this work it is difficult to produce a general evaluation of the performance, as the neural networks have been trained on several instruments data, which also have different characteristics and thus result in different behaviour. For this case the evaluation has been made on an example on which the behaviour for e.g. acoustic guitar could be observed. This is also the case when evaluating the interpolation based synthesis, as here this was also done on an example combination of two instruments. Especially observing and evaluating the behaviour of all instruments and their combinations regarding synthesis, would be a relatively time-consuming task. Therefore the findings in this thesis respond solely to the observations being made on the experiments with the selected samples. 

