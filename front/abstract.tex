\chapter{Abstract}

In todays world, machine learning technologies are more and more present in everyday life. Besides the usage for simplifying common tasks those technologies are growing in their significance for entertainment including the audio domain, for example to generate sounds or even music with them. 

Due to the high interest around this field, this thesis focuses on the exploration of applying machine learning techniques such as neural networks to synthesize novel sounds. As there already exist some approaches in this domain, this work emphasizes on the usage of convolutional neural networks shaped as autoencoders. Throughout this work experiments are conducted, that examine the suitability of convolutional autoencoders, that are differently parameterized regarding the convolution type (1D or 2D) but also the stride within the network. By performing those experiments on log-magnitude spectrograms, some further ones are also conducted on log-mel spectrograms for comparison. Those autoencoder networks are trained to reconstruct audio spectrograms. By introducing a step to interpolate the values generated of two audio samples in the smallest layer, the network is forced to construct a novel audio spectrogram. After converting this spectrogram back into time domain, this yields a novel sound containing characteristics of the two original spectrograms. While mainly the ability of convolutional autoencoders to synthesize audio is proven, those experiments also show which configuration but also kind of spectrogram, works out best. This happens in terms of model performance but more importantly on the output quality. 

Throughout the experiments in this thesis it could be observed, that based on the results, different configurations of the network certainly influenced the model performance and output quality. Applying 2D convolutional networks with less strides performed best regarding the quality of the audible output. Otherwise while more strides or 1D convolutions were applied, the audible quality decreased significantly. This also happened when applying log-mel spectrograms in contrast to log-magnitude. Those findings could be confirmed either by reconstructing single spectrograms but also when interpolation in the smallest layer was applied. As a result of this thesis, it nevertheless got proven that by using convolutional autoencoders trained on spectrograms, novel sounds can be generated on the basis of two instruments characteristics.
