\chapter{Related Works}

Despite this technology is not that well explored and popular as in the image domain, there exist a few proposed approaches that have developed a rather good solution. Some of these approaches have proven, that with Neural Networks it is possible to generate synthesized audio up to a certain quality.

\section{Audio Synthesis}
Probably one of the most prominent solution is from Engel et al. \cite{Engel2017}. With their work they have proposed a system that is capable of synthesizing audio as well as interpolating/morphing encoded audio data of two instruments to create new audio. Not only they have proposed a system, but also a public available Dataset called "NSynth" that contains a large scale of high quality musical notes. The latter has been used for training of this specific project. In their work regarding the synthesis, Engel et. al. developed and compared two different approaches with two differ- ent kind of networks. Nevertheless they have a similar structure, as they are both designed as Autoencoders but accept different kinds of data and thus have different components. While the one kind of network operates on time domain data the other one is trained on the spectral representation of audio samples. Throughout their work the second technology using spectrograms is referenced/used as Baseline Model as they focus on the use of so called "WaveNet Autoencoders" that are trained on continuous time signal. With using the Autoencoder-Structure they make use of its ability in learning efficient encodings of the music data. These encodings are representing essential features from the original audio. To create new sounds they take the encoded data from the embeddding space of two instruments and interpolate them linearly. In addition they used the decoder part to reconstruct it back to audio data. With this mechanisms they were able to create some new sounds which contain the characteristics of two different audio signals. The re- sult can be explored via their online AI-Experiment called "Sound Maker".

\section{Audio Style Transfer}
Another approach is proposed in a paper by Ramani et al. \cite{Ramani2018}. In their approach they developed a Neural Network that is also, like the previous paper, based on an Autoencoder Architecture. As also the title says, they speak officially about their system as "Audio Style Transfer Algorithm". The process of generating an audio containing characteristics of two audio signals is here slightly different as in the work of Engel et al.. as they use in order two networks, namely a transformation network and a loss network. At first the transformation network transforms an input spectrogram into a stylised spectrogram whereas it is using the pre trained weights from the loss network. The latter is subsequently used to calculate the style-loss but also content-loss between the respective spectrograms and the output from the transformation network. This loss gets minimized by back- propagation to the transformation network. By this procedure it is possible to pass a single spectrogram through the transformation network which in order outputs a new spectrogram containing the characteristics of itself (content) but also of one other style audio. To be also mentioned due to its architecture it also performs really fast and could be used for real-time use.\\

\textit{Verma et al.} presented in their paper in 2018 a new machine learning technique for the purpose of generating novel sounds \cite{}



Another source is coming from \cite{Liu2019}

\section{Image Style Transfer}
\label{sec:imgstyletransfer}

<In this section briefly mention the approaches of Gatys and Johnson for image style transfer and it's relation to audio style transfer \cite{Gatys2016, johnson2016perceptual}>