\chapter{Related Works}
\label{cha:related_works}

There already exist a few good approaches around the thematic of neural style transfer or generating audios using neural networks, that present rather good solutions. Some of these studies have proven, that with neural networks it is possible to generate synthesized audio up to a certain quality. Those approaches can get categorized into different areas, as their principle and methodology differ in certain ways. As this field is related to the technique of image style transfer, a lot of works apply those methods to audio, and respective audio spectrograms and thus, call it explicitly audio style transfer. Secondly because those solutions, are defining a combination specifically of content and style. This topic will get discussed in more detail in section \ref{sec:rw_audio_style_transfer}. All methods which do not include this principle of content and style, can get categorized to the technique of neural audio synthesis or simply just audio synthesis (see \ref{sec:rw_neural_audio_synthesis}). Those methods incorporate mostly autoencoder networks. 

\section{Neural Audio Synthesis}
\label{sec:rw_neural_audio_synthesis}
Neural audio synthesis is the field of creating or synthesizing novel sounds with the help of neural networks. The approach is similar and related to the field of audio style transfer. As described before, approaches in this domain differ in certain ways to neural style transfer. As a major difference, with respect to neural audio synthesis, no content or style sound is specified, which means, that for the creation of novel sounds, two sound sources are used equally. While audio style transfer also gets frequently applied on whole audio samples or musical pieces, in audio synthesis the focus is more directed on the application for single notes. With a special look onto autoencoder networks, neural audio synthesis also includes the tasks of learning important sound features for compression and recreation of the input data. How different approaches are designed and which machine learning techniques and which results could be obtained, will be described as follows.\\

Probably one of the most prominent solutions, in the field of neural audio synthesis, comes from \textit{Engel et al.} \cite{Engel2017}. With their work “Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders” they have proposed a system that is capable of synthesizing audio as well as interpolating/morphing encoded audio data of two instruments to create new audio.In addition, there also exists a publicly available dataset called "NSynth" that contains a large scale of high quality musical notes. The dataset has been applied for training of the model in the course of this specific project. In their work regarding the synthesis, \textit{Engel et al.} developed and compared two different approaches with two different kind of networks. Basically they have a similar structure, as they are both designed as autoencoders. but accept different formats of audio-data and thus have different components. While the one kind of network operates on time-continuous data the other one is trained on the spectral representation of audio samples. Throughout the work, the second technology using spectrograms is referenced as Baseline Model, by using so called "WaveNet Autoencoders", that are trained on continuous time signal. The Autoencoder-Structure enables learning efficient encodings of the music data, which are representing essential features from the original audio. In order to create new sounds they take the encoded data from the embeddding space of two instruments and interpolate them linearly. In addition they applied the decoder part to reconstruct audio data. As a result they were able to create new sounds with the characteristics of two different audio signals. Comparing the performance of the two different networks used, they found the WaveNet-style autoencoder to be advantageous. This was proven both by the error scores for reconstructing the audios or auditory quality and by quantitative comparison with a pitch and quality classifier model. Nevertheless it was also concluded that the spectral baseline model has a strong performance.\\
The results regarding the WaveNet Autoencoder can be explored via Engel's online AI-Experiment called "Sound Maker".\footnote{"Sound Maker" \url{https://magenta.tensorflow.org/nsynth-instrument}}.\\

In further publications and approaches, \textit{Engel} continued the research on neural audio synthesis by applying other network structures for this purpose. With respect to generative adversarial networks (GANs) and recurrent neural networks (RNNs), two more works have been published in the field of neural audio synthesis. \cite{Engel2019, hantrakul2019fast} Similar to their work concerning WaveNet-style and convolutional autoencoder, they conducted experiments in (re)synthesizing audios, as an example interpolation of extracted features has been done. Eventually the discuessed works, show a suitability for audio synthesis and also highlight a major speedup in the computation of synthesized audio samples.\\

The work by \textit{Natsiou et al.} does not explicitly mention the term neural audio synthesis in its title, but deals with it throughout the article. \cite{Natsiou2023} In their work they do a research on the reconstruction capacity of (stacked) convolutional autoencoders in terms of log-mel-spectrograms and carry out experiments on different configurations. In their experiments they evaluate the effectiveness of autoencoders in terms of neural audio synthesis whereas also feasible improvements through additional techniques are measured. As well they mention, that their work makes an exploration on musical timbre compression. Here the synthesis gets specifically referred to timbre synthesis. As audio spectrograms exist with different scales, this approach uses, in contrast to others, the log-mel scale. This scale proves to be beneficial, as it already captures the most significant properties with the effect of consuming less memory and computational power. For the training the autors used the NSynth-Dataset proposed by \textit{Engel et al. } \cite{Engel2017}, whereas just a sub-sample, containing samples of different instruments of one single pitch, was considered. The model(s) that were used throughout their experiments, followed the general structure of a (stacked) convolutional autoencoder network, which is composed mainly of 2D convolutional layers. For experimental reasons, additional layers and techniques such as pooling layers, fully connected layers, dropout, kernel regularization got applied (added/removed). In order to measure the results of their experiments they applied error metrics such as root mean squared error (RMSE) and structural similarity index (SSIM). As a result these metrics cannot accurately say anything about the quality. Because of this reason, they also introduced a precision and recall score and combined it in a F1\_score. In order to generate sounds from the spectrograms, they were utilizing the preserved phase information, unless there was no modification of the embedding. In the latter case, the Griffin Lim phase estimation algorithm was applied, as no phase information is present.\\
Regarding the results that could be obtained by reconstructing spectrograms (without modification in latent space), some interesting findings could be extracted. To their surprise, by reducing the size of the latent space, they found out that the smaller it is, more accurate spectrograms with a smoother distribution could be generated. Also, in some cases where kernel regularization got applied, the spectrograms were more accurate, while with dropout layers no improvement could be achieved. The use of (max) pooling also resulted in a more accurate time-frequency resolution with less noise than with just convolution layers. Finally, after the removal of the fully connected  layers it showed that the quality was significant better, as spatial information got preserved better.\\

Regarding audio synthesis, \textit{Colonel et al.} published a few works, where they investigated the suitability of autoencoder networks in connection with audio synthesis. \cite{colonel2017improving, colonel2018autoencoding, Colonel2020} Starting in 2017 they proposed an autoencoder based audio synthesis through compression and reconstruction of audio spectrograms. \cite{colonel2017improving} In contrast to the before mentioned approaches, this one is based on fully connected layers without convolutions. Also a self-made dataset was created, with their own synthesizer. In contrast to to e.G. the NSynth dataset, this one contains polyphonic notes and thus more complex harmonies. During the experiment they trained different parameterized networks, where they vary the depth and width of the network and its layers as well as the activation functions and different optimizers. As error metric in this work the mean squared error (MSE) was used. Comparing these scores regarding networks of one or two hidden layers on each side show, the network using the Adam optimizer worked out best in contrast to using Momentum as optimizer. These networks, based on sigmoid activation functions worked best when less compression is applied. The concept of 4 hidden layers, showed having a mix of ReLU and sigmoid activation functions worked out best. Additionally by applying regularization methods such as dropout and l2 penalty, the latter proved to be better, as the results where of better auditory quality. Further results showed that sigmoid activations led to fuller sound than with ReLU. Furthermore by using bias terms, it could be observed that noise was present in the results. As a consequence, despite of the better convergence, they chose to let them out. The authors concluded, that using a network with 4 hidden layers and a composition of sigmoid and ReLU worked out best also in terms of auditory quality.\\

Another work by \textit{Colonel et al.} was proposed in 2018, which actually states an improvement of the method, described in their previous work from 2017. \cite{colonel2018autoencoding} Those improvements contain the use of a phase reconstruction method not used before, which allows a direct activation of the latent space. For an improved model convergence, the autoencoder was designed asymmetrically, via input augmentation. This means that they padded the input magnitude data with different permutations, being first or second order difference or mel-frequency cepstral coefficients (MFCC). Whereas in the previous work only MSE was contemplated as error metric, here, a comparison of several cost functions was used. Within the cost functions, the mean absolute error (MAE), as well the as spectral convergence cost function (SC) with L2 penalty, was considered. The penalization of the total spectral power, proves to be advantageous as the power in the output is more accurate as compared to others. In comparison to their work from 2017, they also left out additional bias terms, but decided to just use ReLU-activations instead of a mixture with sigmoid.\\
Eventually improvements to their previous results could be achieved regarding the additional methods, that were applied. Concerning the augmentation of the input data, a significant improvement regarding score could be reached, whereas augmenting with first order difference outperformed all other. With a look onto the generated sound, it could be observed, that by padding with the MFCCs a different sound palette was present. In another comparison to their baseline, \textit{Colonel et al.} introduced the possibility to omit the encoder part of the network. This directly enables the activation of the innermost 8 neuron layer while the decoder can generate novel sounds. As no phase information was present, estimations were done via a method called real-time phase gradient heap integration, which enables the generation of a playable sound. In addition to this work, the authors implemented a small program including a GUI, where it is possible to directly interact and activate the innermost neurons (eight control values in latent space) to generate new sounds.\\

In a more recent work, \textit{Colonel et al.} implemented and compared autoencoder networks with different topologies regarding their performances for musical timbre generation. \cite{Colonel2020} This work already utilizes findings and methodologies from previous works. Based on a study from 2018, they implemented a mechanism to directly activate and control the latent space of a trained autoencoder with a graphical tool. They found out that this technique proved to be difficult in terms of controlling the latent space. To overcome this issue and improve the work, they added chroma-based input augmentation to improve the reconstruction performance in this approach. The chroma-values are based on the 12 note (western) scale to represent the dominant note present in an audio sample. Besides this type of input augmentation, they also implemented a so-called skip connection, where the latent space gets conditioned with the chroma-value. In this work the chroma-values get represented via a one-hot encoded representation for each training sample, where the maximum value is set to one while all others to zero. Consequently these one-hot encoded chroma representations tell the note played in a single-note audio. With this technique the authors could shape the timbre around a specific note class. For the networks topologies, they varied the size of the "bottleneck"-layer (8, 3 or 2 neurons), the activation functions, the input augmentation, the use of the chroma skip connection, as well as different datasets. It should be mentioned that the authors trained and experimented with the self-generated dataset from their previous works, containing five octaves of notes, a one octave subset of it and a separate violin note dataset.\\
As a result, the network with an eight neuron bottleneck, with the chroma-based input augmentation worked out best. Thus, for the rest of the experiments \textit{Colonel et al.} were using this technique. In case of the two neuron bottleneck network, the sigmoid activation functions without skip connection worked out best for the one octave dataset. The skip connection turned out to work best for the violin dataset (sigmoid and two neurons). Finally in the case of three neurons also the variant with the skip connection worked out best for both datasets. By analyzing the latent spaces some interesting observations could be made for the sake of audio synthesis. The authors applied a clustering method to see the distribution of the values in the latent space concerning their note and timbre. Using sigmoid activations turned out to bind the values in the range of (0,1) as well as distributing the values in a more uniform manner. Also the skip connections led to a denser representation. By taking this as an advantage, and moving forward with just sigmoid activations, sampling of the latent space (with a mesh grid e.g. 350x350 for two neurons bottleneck) was done to generate a new timbre. In combination with setting the additional chroma conditioning vector to a given note class, the decoder generates the timbre that matches the chroma vector and thus, the desired note is present in the output sample.\\

A comparative work on autoencoders, in terms of music sound modeling, has been published by \textit{Roche et al.} in 2019. \cite{roche2019autoencoders} In this work they implemented four different types of autoencoder networks, that have been compared in terms of audio synthesis. Similar to the other techniques described earlier, this one also orientates itself on the principle of an autoencoder, to project the input data to a low-dimensional space, from which input can be (re)synthesized. In the described experiments, the proposed autoencoder networks consist of (shallow) autoencoers (AEs), deep autoencoders (DAEs), recurrent autoencoders (LSTM-AEs) and variational autoencoders (VAEs) which all got compared to principal component analysis (PCA) as baseline. As sound data for training and experimenting, they used a subsample of 10,000 different random selected notes from the public available NSynth dataset. The networks that where implemented got trained on the normalized log-magnitude spectra of those samples. Regarding the structure or the depth of the different networks, the researchers used two and three layers for the DAE on each side. In the case of the VAE just one version with two layers, and one version of the LSTM-AE with one layer on each side was applied. Regarding the size of the output from the encoder (latent space), they experimented with different values in a range from 4 to 100. The conducted experiments consist of an resynthesis-analysis where the reconstruction error (RMSE in dB) of the different methods got compared. Additionally to the RMSE so called PEMO-Q scores were introduced to calculate the objective measures of perceptual audio quality.\\
The results showed to their surprise, that PCA outperformed the shallow autoencoder network. Continuing with DAEs, the reconstruction performed almost 20\% better than the shallow AE, having an encoding size of 12 and 16. Also the error decreased faster when the dimension of the latent space was decreased. Even better results with over 23\% improvement compared to PCA could be achieved by using LSTM-AEs which brought them to the concolusion, that it is feasible to use more complex architectures. The fact that more compression and thus a small latent space can be generated, is even more important for sound-synthesis. In comparison the reconstuction error from the VAE was lying between the one of the DAE and shallow AE/PCA. As the size of the latent space influences the reconstruction error, it can be stated, that the bigger the size, the lower the error. Interestingly PCA outperforms all models having an encoding size of 100. In addition to the RMSE score, the perceptual audio quality got measured with te PEMO-Q score. The results are comparable to those with RMSE, with just the LSTM-AE having a slightly lower score as compared to RMSE. In this context it was also investigated how the latent space values can be used to be controlled by musicians, and thus, the correlation between those values has been calculated. Averaged over all samples per model, it was shown that the values from LSTM have the most correlation while VAE has the worst. Having less correlation makes VAE the better candidate in terms of using the latent values as control values for synthesis (less redundancy and clear perceptual meaning). In terms of audio synthesis, including the latent space variables, \textit{Roche et al.} also demonstrated how it could be applied for sound interpolation like in the Work of \textit{Engel et al.}. For this task they selected the latent space vectors of two sounds with different characteristics and linearly interpolated each value. By decoding and in addition, applying the inverse STFT and Griffin Lim, new interesting sounds could be generated. 

\section{Audio Style Transfer}
\label{sec:rw_audio_style_transfer}
The below discussed works, all orientate themselves on the techniques of image style transfer. As those techniques have a significant impact on the development of audio style transfer algorithms, two important works get discussed in section \ref{sec:rw_imgstyletransfer}. Applying the method of image style transfer to audio also means, as audio is a time-continuous signal, that it has to be brought into a similar shape, which will be done mostly by generating spectrograms out of signals. As for image style transfer, a content and a style picture is needed, this principle also gets applied to audio style transfer. In image style transfer, the style (e.g. brush strokes, colors) and content of an other image (e.g. contours, scenery) get combined, to form a new stylised image. \cite{Gatys2016} This means that in the output image, the content image looks painted with a certain "style". Mapping this principle to the audio domain, this means, that there has to be a specific content sound (sample) that gets stylized with a certain style of a sound (e.g. style of a specific instrument). In the image domain it is difficult to distinguish content from style, whereas it is a bigger question that appears in the different approaches. Most authors define the style as a musical instruments' timbre or even a musical genre. Alongside, the content might get defined as global music structure with rhythmic constraints. \cite{Grinstein2018} Those scientific questions also might be influenced if whole audio samples/musical pieces might be taken to get stylised or just some single notes from an instruments. Furthermore if speech is considered as audio data, style and content differently defined as well. Here, style could be e.g. the emotion of the voice or the speakers identity and content the spoken words in an sample. The following works show different solutions specific to the problem of Audio Style transfer in which they also get compared and assessed. \\

One approach that applies this principle, is the solution proposed by Ramani et al. in 2018. \cite{Ramani2018} In their study they developed a neural network that is constructed as an (convolutional) autoencoder. Here they described officially their system as audio style transfer algorithm. In this case the process of generating an audio containing characteristics of two audio signals is slightly different as in the work of \textit{Engel et al.}. They worked with two networks, namely a transformation network and a loss network. This architecture and methodology is especially inspired by the neural style transfer algorithm by \textit{Johnson et al.}. Both networks have the same structure and composition of layers. The loss network is trained to compress input spectrograms to lower dimensions, which means that the encoder part learns to preserve the high level features of the input. In addition, the decoder learns to reconstruct a spectrogram similar to the input of the network from the encoded data. For the training of the transformation network, the pre-trained weights of the loss network are used which speeds up the learning process. This means just optimization towards low level features/style has to be made. The trained transformation network, is then able to transform an input spectrogram into a stylised spectrogram. The loss network is subsequently used to calculate the style-loss but also content-loss between the respective spectrograms and the output from the transformation network. This loss gets minimized by back- propagation to the transformation network. Through this procedure it is possible to pass a single spectrogram through the transformation network. In the following the network outputs a new spectrogram containing the characteristics of itself (content) but also of one other style audio. Due to its architecture it also performed really fast and could be used for real-time use.\\

\textit{Verma et al.} presented a new machine learning technique for the purpose of generating novel sounds, in their paper in 2018 \cite{verma2018neural}. In this approach they tried to apply the method for artistic image style transfer, into audio and they specifically mention the approach proposed by \textit{Gatys et al.}\cite{Gatys2016} (see section \ref{sec:rw_imgstyletransfer}). Unlike Gatys image style transfer approach, they adapted and trained an AlexNet architecture on the classification of audio-samples. This kind of network is a so called convolutional neural network, where the audio gets converted into spectrograms, which can be seen as grey-scale images. An important aspect is, in this work they used the log-magnitude data of the STFT output. It also should be mentioned, that they adapted the network towards a receptive size (kernel) of 3x3 instead of the larger ones in the original network, as it retains the resolution of the audio. Similarly to the image domain, the stylised output image gets initialized with random noise. Thus an input spectrogram consisting of a gaussian noise signal is utilized. This one gets iteratively optimized by minimizing the content, but also style loss via back-propagation. In the end this process creates a spectrogram combining the content of one audio with the style of one other audio sample. Additionally they found out that including additional loss terms for temporal and frequency energy envelopes, helped to improve the quality, as otherwise temporal dynamics would not get incorporated. For their experiments they imposed the style of a tuning fork onto a harp sound and also transferred the style of a violin sound onto a sample of a singing voice. In this way they developed a novel method for achieving cross-synthesis by using image style transfer methods.\\

More work was published by \textit{Liu et al.} \cite{Liu2019} exploring the application of technologies given from the image domain for "mixing audio". This means, that this approch focuses on using audio as spectrograms. As the previous study solely focused on the one technique by \textit{Gatys et al.} this experimient explores two more approaches. While one inspired by \textit{Johnson et al.}, a convolutional autoencoder coupled with a VGG classification network, the other one uses an approach with (cycle)GAN (Generative Adversarial Network). In this work they call Gatys' approach specifically "slow transfer", as the iterative computation from gaussian noise was proven to be really slow. Different to the previous work by Verma et al., the authors adapted a VGG network (1 input channel in first layer instead of 3) for the "slow transfer" method. This approach has also been used in Gatys' image style transfer. The transfer process is also similar to the previous work, as Liu uses a spectrogram initialized as gaussian noise in order to iteratively minimize the content loss in the higher layers, and the style loss in the lower layers. Setting this transfer process as a baseline model, they also adapted a faster style transfer method by coupling the VGG network with a convolutional autencoder network. The purpose of this network is to take the content spectrogram as input and in the following outputting a spectrogram containing also the style features of a style spectrogram. Comparing the described work to other approaches, this proves to be similar to the one of Ramani et al. having a transformation network. The only difference is the second network as here they are using a VGG classification network and no second autoencoder. Having the output of the autoencoder network (also called generative network) this one serves as the initial spectrogram on which the content and style loss gets computed in the VGG network. The gradient descent then gets applied to the autoencoder network, resulting after few iterations, in a stylized spectrogram. The researchers have proven that their approach is faster than the one with gaussian noise. As mentioned before, for the third experiment they adapted a cycleGAN, which accepts audio spectrograms instead of images. In the image domain, this kind of network is able to apply style transfer to only a portion of the input images. Secondly the application of this method generates two images as transfer is done in both directions, which means in case of audio that two new sounds get calculated. As another result, this study showed that their approach generate te results in a shorter amount of time. For the purpose of evaluation, they listen to the outcome, but also apply objective mechanisms like visual assessment of spectrograms, consistency tests with classification and examination of signal clusters. Eventually the authors state that with the baseline approach features like the harmonic is not clear and high frequencies get discarded and that the faster transfer emphasizes on lower frequencies but is missing out on beginnings of the notes. With cycleGAN also the lower frequencies get emphasized while higher ones get discarded. The listenable results of each approach are provided online.\footnote{\url{https://www.xuehaoliu.com/audio-show}}\\

As the above mentioned approaches are working on single notes, the experiment of \textit{Grinstein et al.} has been implemented for whole audio samples \cite{Grinstein2018}. Within their work they were adapting several other approaches, with neural networks from the image domain. Besides of neural networks, they also implemented a handcrafted sound texture model which got compared to the neural approaches. The latter one is composed of three sound processing steps, that in combination emulates the human auditory system. Taking a closer look on their work, especially on the neural networks, it can be said that they differ from existing ones in several ways. On the one hand, they do not use a random noise spectrogram, but use the content spectrogram which then gets stylised through their methods. On the other hand many audio style transfer approaches, explicit are computing the result with a combined loss function, that incorporates both style and content loss. \textit{Grinstein et al.} do not make use of this concept, as they already initialize the future stylized spectrogam with the content spectrogram, like mentioned previously. On this target spectrogram, just the style loss gets optimized, as the content is already present. To mention here, they proved this method to have compelling results, as the global structure of the content sound gets preserved.\\
In detail the neural network-based approach the authors investigated the use of three different network architectures for the purpose of audio style transfer. Concerning all three network types, they minimized the style loss on the content sound respective spectrogram. This style loss is equally computed as in Gatys' image style transfer approach. It gets calculated by minimizing the error to a "style sound or spectrogram", at specific layers in the network that extract the style. Via back-propagation the loss gets minimized again at each layer, which results in a stylised content sound or spectrogram, after a few iterations. This workflow was applied to all three different network types and compared in the end. As first network they used a VGG-19 network like Gatys, where the input spectrogram was replicated three times in order to mach the input shape (RGB-like). By averaging all three channels in the end, they were able to obtain the final stylised spectrogram. The second network is called SoundNet, which is a convolutional network learned on unlabeled videos including sounds. This type of network operates on the raw waveform where no generation of spectrograms has to be done in advance. Finally a wide-shallow-random network was investigated with audio spectrograms consisting of just one-layer CNN (like in the work of Ulyanov and Lebedev \cite{ulyanov2016audio}). As the fourth and last method they made trials with a handcrafted sound texture model that emulates the human auditory system. Even it is no neural network, it consists of three layers doing cochlear filtering, envelop extraction with compressive non-linearity and modulation filtering.\\
Eventually the authors came to the following experimental conclusions: While using the VGG network no meaningful results could be obtained due to the noisiness. In comparison the SoundNet yielded more relevant results despite also containing some noise. Surprisingly the shallow random network performed best together with the sound texture model. For a better understanding, the results were provided online.\footnote{\url{https://egrinstein.github.io/2017/10/25/ast.html}}\\

The work of Ulyanov and Lebedev has to be mentioned here, as they are often referred to be one of the first, that explored transfer algorithms for audio. \cite{ulyanov2016audio} In fact, they took the architecture used in image style transfer by Gatys et al. and adapted it for the use on audio spectrograms. Rather then seeing the spectrogram as a picture, with the dimensions of frequency x time, they took the freuquency values as channels for the CNN. The network itself was designed as a shallow network (1-layer) using 1D-Convolutions with random weights. To obtain a final spectrogram containing content and style, an optimization is made on random noise, to minimize the loss values to a style and a content spectrogram. Instead of applying it on single notes, this approach also uses longer samples or music snippets. 

\section{Image Style Transfer}
\label{sec:rw_imgstyletransfer}
In the previous sections it has been written about neural audio synthesis as well as neural audio style transfer. A lot of these works especially those proposing solutions for neural audio style transfer, took their inspirations from the image domain. For this reason this section makes a short excursion relevant image style transfer works of \textit{Gatys et al.} as well as \textit{Johnson et al.}. \cite{Gatys2016, johnson2016perceptual} \textit{Gatys et al.} were the first to implement a system of neural style transfer applied on visual data. By using a convolutional network trained on object recognition and localization (VGG-19) in images, they were able to extract the texture as style but also the content of an image. They found that especially from higher layers in the network, high-level features of the images, can get reconstructed. This includes objects and their arrangements in the scene, without the exact pixel information, which will be used as content representation further on. Using a special feature space for texture synthesis, the style of a content image can get extracted by using the feature responses at certain layers in the network. By combining these two principles, respective style losses and content loss can get computed which will be used for the style transfer. The generation of the target image starts by initialization of a random noise image, on which those losses get minimized by using gradient descent.\\
\textit{Johnson et al.} developed an improved image style mechanism on the basis of the former methodology, that particularly shows improvements regarding computational speed. For the computation of content and style losses they use a VGG network with 16 layers and pre-trained on image classification. Here they added a special transformation network, that is designed as autoencoder. This one takes a target image as input (content image) and synthesizes an image on which the style and content loss gets calculated in the VGG network instead of a random noise. Back-propagation was performed just in the transformation network, while the VGG network stayed fixed. As a consequence the transformation network produced a stylised image after training. By comparing Johnson's work to the method by \textit{Gatys et al.}, it shows a significant improvement regarding computational speed but also yields promising results.\\
Previously described image domain methods significantly inspired the development of audio style transfer algorithms, which are presented in section \ref{sec:rw_audio_style_transfer}. Finally, it should be noted that works by \cite{Ramani2018} and \cite{Liu2019} adapted and applied the method of \textit{Johnson et al.} while all other in section \ref{sec:rw_audio_style_transfer} mainly used the methodology proposed by \textit{Gatys et al.}.