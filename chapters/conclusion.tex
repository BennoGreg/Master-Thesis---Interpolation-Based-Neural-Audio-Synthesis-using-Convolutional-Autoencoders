\chapter{Conclusion and Future Work}
\label{cha:Conclusion}

This thesis and the work underlying it dealt with the exploration of using machine learning methods such as neural networks for the generation of audio data. As the title of this thesis states, this work emphasized on the use of convolutional neural networks that are shaped as an autoencoder. Due to this shape, the first part (encoder) of these networks produces a projection of the input data to a lower dimensional space, from which the second part (decoder) is able to regenerate the original input, which in this case are audio spectrograms. By modifying this latent space through interpolation, interesting findings and results could be obtained which are relevant for the field of neural audio synthesis. Not at least as they give important findings regarding the use of spectrograms and convolutional networks to synthesize audio. Having the final evaluation, the research questions defined at the beginning of this work can be answered.

First of all as discussed in the previous chapters \ref{cha:Discussion} and \ref{cha:Results}, it was proven that by the application of convolutional autoencoder networks, novel sounds can be generated, through interpolating the embeddings of two different samples. The networks, that used log-magnitude spectrograms and 2D convolutions with single- or double-strides, showed the most promising results. Promising in the way that the output spectrograms and moreover the output sounds contain the characteristics of both input samples best in a novel sound. Moreover those sounds also have a good listening quality without noise or distortion. By having evaluated different types of networks containing 1D or 2D convolutions or different amount of stridings it also can be answered, that the configuration and composition has a significant impact on the model performance but also the quality of the output. Especially to mention the 1D convolutional network and the triple-strided 2D convolutional networks as there too much information is lost and distortion is included. Further to mention the reconstruction or synthesis of lower notes in combination with Griffin-Lim phase estimation, leads to the loss of the pitch. Nevertheless as this is a general problem, which appears among all network types, it gets influenced by the amount of striding. Not only the model configurations influence the quality of the output, but also the pre-processing as seen in the evaluation. With special respect to applying log-mel spectrograms compared of log-magnitude, the overall model performance but also the final sound is of a lower quality. All these findings can be concluded for either reconstructing single samples but also with the introduced interpolation step. Regarding the learnt information within the neural networks, it can be concluded that the networks mainly learn to project the harmonical structures of an input spectrogram onto a lower dimensional representation. Finally, regarding the question whether audio spectrograms are suited best to synthesize audio, it can be concluded, that despite generating already promising results, limitations and disadvantages occur. This is especially true when aiming to research towards a real-time capable system. 

Despite initial difficulties regarding the ability to properly train a complex convolutional network, with a large-scale dataset, those could be overcome and resulted in a promising outcome. The evaluation of this outcome also turned out to be difficult, as audio data is the main output and thus is difficult to display on paper. Nevertheless by displaying the outcome as error scores, spectrograms, embeddings or signal plots and evaluating them in combination with listening relevant findings could be gained. While those findings originate of experiments based on a small subset of the dataset, these nevertheless give an important insight on how a convolutional neural network can be applied to synthesize audio and what has to be taken care of in terms of the output quality.

\section{Future Work}
Having this work with already promising results, those results still motivate further research. Furthermore to gain a more detailed knowledge on the behaviour and differences of distinct instruments, further experiments across more instruments should be conducted. Not at least as this would provide additional insights on how the performance of the models differ by using different instruments. Further on, like in some related works, experiments by applying different activation functions, error scores or introducing additional layer types like fully-connected should be conducted. Some works in the field of neural audio synthesis also introduced pitch conditioning techniques, which are also a possible subject to be introduced in future studies. By adding those studies more observations could be made regarding the model performance respective the output quality and if eventual improvements can be achieved.

As the field of neural audio synthesis is of high interest for the generation of audio respective music, an implementation that is capable for real-time usage would be desirable. Despite the fact that this implementation mainly focused on the general ability to synthesize audio, it would nevertheless serve as a solid base to construct such a system.