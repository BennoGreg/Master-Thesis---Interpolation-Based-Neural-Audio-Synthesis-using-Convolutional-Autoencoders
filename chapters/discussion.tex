\chapter{Discussion/Evaluation}
\label{cha:Discussion}

In this chapter the results (see chapter \ref{cha:Results}) that could be obtained by the experiments shown in chapter \ref{cha:Experiment} will get discussed. As the results of the experiments cannot solely be described through graphics and numbers shown in the previous chapter, the discussion also takes the auditory results into account. Based on the combination of the graphical and auditory results, some interesting findings and observations can be derived that get discussed in the following sections.

%\section{Interpretation of results / observations}

\section{Impact of model configurations}
Throughout the experiments different models have been trained and utilized for reconstruction or synthesis of audio spectrograms. With these experiments, the goal was to find a model that is suited for the task of (re)synthesizing audio data. Furthermore by exploring different models with different configurations, an insight could be gained, on how these affect the performance but also the output of the model. In the case of this work, the results, of the experiments, can be presented in different ways. In the previous chapter \ref{cha:Results}, the results have been depicted as error scores, but also the embedded space data and the resulting spectrograms. With those alone, the approaches cannot be fully evaluated, as it is also important to consider and discuss the auditory quality, as it dealt with audio data. The following section therefore takes the numerical and graphical but also auditory results into account to evaluate the models with regard to their configuration. In total the evaluation takes the results of 7 different models into account.

\subsection{Impact on score}
\label{subsec:disc_conf_score}
To train a neural network, but also other machine learning models, an error score is needed. With the help of this score, the network gets optimized to in reverse minimize the error and thus improve the networks performance and quality of output. In chapter \ref{cha:Results} the error scores regarding each network configuration are displayed separately regarding the network type (1D or 2D convolutional) and input type (log-mel vs. log-magnitude spectrograms).

By comparing the error scores regarding the network type it can be said, that the 1D convolutional network performed best, as it has the lowest scores on training and validation, but especially testing. All networks that use 2D convolutions and operate on snippets coming either from log-magnitude or log-mel spectrograms have a worse score. To mention those scores just show the amount of error between the reconstructed and the input data. Furthermore when also considering the amound of strides beeing used an interesting observation can be found regarding the score. Here it can be said, that the more striding is applied in the network, the higher gets the error. This was just observed using the 2D convolutional networks, as with the 1D convolutional networks no strides have been applied. Mentioning also the scores concerning specific pitch classes, it can be said that the 1D convolutional network has the best scores in all pitch classes, being all constant regarding their pitch. Moving on to the 2D convolutional networks, the scores of the single pitch classes are also higher than the ones from the 1D convolutional network. Those also increase with the amount of striding. Nevertheless it can be seen that, concerning the 2D convolutional networks with log-magnitude spectrograms and 1D-convolutional networks, they all have constant values regarding the pitch. An exception make here the three networks trained on log-mel spectrograms as those increase the error with the pitch class. As this is mainly influenced by the type of input, this gets also mentioned later on in section \ref{sec:disc_imp_pre_processing}. 

In total it can be said that 1D convolutional networks yield a better score than 2D convolutional networks having also the best scores among specific pitch classes. Furhtermore regarding the 2D convolutional networks it can be said that the amount striding certainly influences the models score, as the more striding is applied, the higher gets the error in the network. Nevertheless alone of this score, it cannot be concluded that the output sound using a 1D convolutional network is also of better quality. This can also be said concerning the stridings within the network, as the output has to be evaluated further on by listing to the final sounds. Furthermore as the error scores for the specific pitch classes, does not increase (models with log-magnitude spectrograms), it also cannot be said, that the output of all pitch classes is equally good in its auditory quality. Therefore in the next section \ref{subsec:disc_conf_single_rec} the outputs being spectrogams, embeddings but also the generated audible signals, get discussed in order to come to a conclusion regarding the impact of the models configuration.

\subsection{Impact on single reconstruction}
\label{subsec:disc_conf_single_rec}


As in the previous subsection \ref{subsec:disc_conf_score} the impact of model configuration on the score has been discussed, here the generated experimental artefacts such as spectrograms, embeddings and audios get included into the evaluation. Like in the results-chapter \ref{cha:Results} the discussion here gets divided into two parts, whereas in this part the impact on the single reconstruction and in the following part (subsection \ref{subsec:disc_conf_inter} the impact on the interpolated (synthesized) outcome gets assessed. 
This subsection also mainly deals with the outcome of the networks using log-magnitude data. The discussion regarding the log-mel spectrograms, will follow later on in section \ref{sec:disc_imp_pre_processing}.


\subsubsection{Impact on generated spectrograms and audio}
In this point the artefacts like spectrograms and audios gained by reconstructing single audios without altering the embeddings, get assessed. This should bring insights into how the model configuration affects the ability to reconstruct audio spectrograms but also the quality of the output. As an instrument which represents the results, the guitar sample shown in the result chapter gets chosen. This guitar sample contains characteristics such as the guitar stroke but also the damping of the strings at the end of the sample. Also it has the characteristic that it fades over time. To take the spectrograms and embeddings shown in chapter \ref{cha:Results} some first observations can be made. First on by comparing the output spectrograms of the 1D convolutional network, and those of the 2D convolutional network, it can be seen that the original areas containing little energy (dark) get better preserved in the 2D convolutional networks. The output of the 1D convolutional network appears to have more energy distributed in areas of little energy. Further on it also can be observed that especially the single stride 2D convolutional network, is able to preserve the broad band spectra at the beginning and the end of the spectrograms. In comparison the 1D convolutional network, has difficulties in preserving those broad band spectra. This can be also justified when listening to the final audio. As a note, unless it is mentioned, the evaluation on the audible output is done on samples generated with the original phase information. Concerning the output of the 1D convolutional network the stroke and damping is not present but it is in the ones coming from the 2D convolutional networks. Also the sample of the 1D convolutional network has a worse listening quality as it gets louder from the beginning on and contains noise. The latter especially makes sense as the spectrogram contains more energy across all frequencies that have usually little energy contained. Further on by listening to the outputs of the 2D convolutional network, those are overall better in their audible quality but also when looking onto the spectrograms. Especially concerning the single-stride network its output has the best quality, as it preserves the important characteristics such as stroke and the damp best, but also fades like the original sample. By comparing especially this one, hardly no difference can be noticed, except a small lack in the intensity of the stroke. When listening to the outputs of the double- and triple-strided network, no significant differences can be noticed, as also characteristics, like stroke, fade and damping are still contained. Just the output of the triple-stride network, is slightly distorted, being not as clear as the output of the others, when listening closely. Taking also its corresponding spectrogram into account, those distortions can be justified by the less precise harmonics. Similar observations can be also made by using a sample of a different instrument (e.g. brass). Despite this one does not fade over time and does not have characteristics like a guitar (stroke, damping), the output is also of similiar quality like with the guitar. Also the same conclusions can be made, as the quality using the 2D convolutional network is better than with the 1D convolutional network, having equally good sounds using single- and double-striding. 

As those samples were not altered in their embedding, the original phase information could be reused for the inverse STFT to obtain the sounds. Those have been taken into account in the last paragraph. For the comparison but also as for synthesized spectrograms no phase information is present, samples have been generated using the Griffin Lim algorithm \cite{Griffin1984} to estimate the signal. Using the same guitar sample that has been used for discussion before, using the Griffin Lim algorithm introduced some differences in the sound quality especially concerning the 2D convolutional network. Taking again the output of the 1D convolutional network into account, by listening tho the signal, the quality does not change. Interestingly while listening to the samples generated by the 2D convolutional networks, drawbacks between the ones using Griffin Lim and the others with the original phase information, can be observed. As the ones with the preserved phase fade clearly towards the end, the ones being estimated by Griffin Lim, oscillate slightly in their loudness.

Previously in subsection \ref{subsec:disc_conf_score} the scores regarding their pitch class have been discussed. As those cannot solely be considered for grading the performance and output quality, the audible outputs also are taken into account. As this cannot be done for all samples among all instruments, guitar acoustic samples were picked for evaluation. When listening to the generated samples of the 1D convolutional network, it can be noticed, that among the pitch classes (going from 030 to 100) all the samples have the characteristic in becoming louder towards the end. Furthermore also especially when the pitch gets higher, that the samples contain more distortion. If the final sound gets reconstructed with the Griffin Lim algorithm, an interesting behaviour can be observed. With special regard to the lower pitched samples e.g. midi note 030, the pitch is not recognizable in the output. Further on when using samples of higher pitch classes, the pitch becomes recognizable. 
Coming to the samples generated by the 2D convolutional network, here the output samples of the single- and double-stride networks, remain the same in its quality alongside the pitch. Also no difference between the two networks can be perceived. Regarding the triple-stride network, more distortion can be perceived in the lower pitched samples, while when ascending in the pitch, the distortion gets less and less perceptible. Again when taking the results obtained by applying the Griffin Lim algorithm, the pitch cannot be recognized in the output samples using lower pitched samples. Especially when applying more striding like in the triple-stride network, this behaviour appears more significantly. Having the original phase information this would not be an issue, but in the case of synthesis where the output spectrogram gets reconstructed from an altered embedding and no phase information is present. 

\subsubsection{Impact on generated embeddings}
Finally after discussing the impact of the model configuration on the spectrograms but especially outputs, it is also of interest, how the embeddings differ from model to model. Especially as those represent the features from which the decoder part reconstructs a spectrogram. In chapter \ref{cha:Results} the embeddings of a guitar sample have been depicted together with their corresponding recreated spectrograms. First of all it can be seen, that all embeddings in general are a projection of the input spectrograms on a lower dimensional space. Different to the embeddings generated by the 2D convolutional networks, the ones from the 1D convolutional network preserve the high energy areas with negative values and vice versa. As also in the output spectrograms and samples of the 2D network, the broad band spectra are preserved better, those can be also seen in the embeddings at the beggining and the end. This happens in contrast to the 1D convolutional network, as there like in the output, the broadband spectra are not recognizable. With a final look onto the spectrograms generated by the 2D convolutional networks, it can be seen, that the more striding is applied the less values are available to preserve the inputs features. With regard to the triple-stride network, the embedding contains the essential features of the input, but it regarding its output it lacks to preserve the desired quality which was discussed before. Therefore in this case, the embedding contains little information to reconstruct a spectrogram with a desired quality. This also makes the single- and double-stride network favorable as they preserve enough data to reconstruct spectrograms with a sufficient quality.


\subsection{Impact on interpolation}
\label{subsec:disc_conf_inter}
In subsection \ref{subsec:disc_conf_single_rec} the developed models, got evaluated towards their ability to reconstruct single audio spectrograms. There already promising results and interesting findings could be obtained that are important for synthesizing audio. Again like previously in this section the 1D convolutional but also the single-, double- and triple-stride 2D convolutional networks using log-magnitude spectrograms get taken into account. The evaluation for synthesizing sounds with networks taking log-mel spectrograms as input, will be done later on also in section \ref{sec:disc_imp_pre_processing}. Discussed in this section will be the artifacts like embeddings, output spectrograms, but also output sounds. Furthermore during the experiments correlation coefficients between the embeddings were calculated to obtain interesting combinations of embeddings that differ in their characteristics. This was not done with the embeddings of the 1D convolutional network. Nevertheless interesting combinations for the audio synthesis could be found. For the purpose of these experiments, two instruments (guitar acoustic and brass acoustic) were selected that have significant differences and should synthesize an interesting sound. 

Regarding their correlation coefficients those have been 0.16 (single-stride), 0,0052 (double-stride) and -0,17 (triple-stride). By comparing those values to other combinations, those signalize a low correlation and thus being highly different. As a note it could also be examined that some examples being of the the same instrument family have a high correlation which means they are similar and contain similar characteristics. With this knowledge it can also be approved, that the networks learn to capture the most describing features of the input sample. Some correlation matrices can be seen in the supplementary material to the thesis as they cannot be depicted sufficiently on paper.

\subsubsection{Impact on generated artefacts}
When taking a look on the results in chapter \ref{cha:Results} besides the embeddings and spectrograms concerning single reconstructions also the results of synthesized spectrograms are shown. There the generated embeddings of two instruments were shown in combination with the resulting interpolated embedding. Alongside those, the generated output spectrograms are shown, that represent the synthesized audio. Like mentioned before, as source samples a guitar and brass sample has been chosen, to serve as base for the synthesis task. First taking the results using the 1D convolutional network into account, there it could be seen that by interpolating the embeddings the resulting new embedding contains the features of both instruments. Nevertheless by comparing it to the both input spectrograms it can be seen, that especially the guitar features cannot be recognized that easily. This behaviour can also be recognized, by looking onto the resulting spectrograms, and comparing it to the two input spectrograms. Further on by taking the results using the 2D convolutional networks into account, improvements can be detected. First of all when looking onto the interpolated embeddings, here the instruments features can be distinguished better, especially the guitar sample can be recognized better. This behaviour also varies having different amounts of striding in the networks. In particular when examining the interpolated embeddings of the double- and triple-stride network the features of both instruments can be distinguished the best. When moving on onto the spectrograms, the best quality in terms of preserving the fine structures of the source instruments delivers the single-stride network. Also contrary to the output spectrogram of the 1D convolutional network both instruments can be recognized better. The output spectrogram of the double stride network, appears to preserve the distinct sounds even better, but lacks in preserving the fine structures and changes of the input spectrograms. With a final look onto the output of the triple-stride network, this one has hardly no fine structures preserved as the the harmonic features over time also appear "washed out" and being not recognizable precisely. To also bring the quality of the audible output into relation some further interesting findings can be obtained. Like also with the single reconstruction experiments, the output of the 1D convolutional network does not contain a guitar stroke, and also proves that the guitar is less audible then the brass sample. Furthermore it is also in its amplitude louder then the original samples and contains distortion which is certainly not desired. In comparison the output gained by the 2D convolutional networks, have a overall better perceptual quality. Interestingly as the single- and double-stride network have differences in the spectrograms, those cannot be perceived in the final output sound making them sound equally. The guitar also can be perceived better with the sound characteristics of the brass sample. Furthermore also the stroke is preserved in the output. As a further advantage to the output of the 1D convolutional networ also no distortion is contained, making it sound clearly. Finally listening to the output of the triple-stride network it can be said, that the quality is again significantly worse then with the single- and double-stride networks. Nevertheless the instruments are both perceptable and distinguishable in the output, but it appears to have missing frequency parts, making it sound more "metallic". The latter can also be braught into relation with the "washed out" harmonics in the spectrogram. Furthermore also the pitch is unstable as it rises in the end of the sample. 




\section{Impact of pre processing}
\label{sec:disc_imp_pre_processing}

Like it has been described in chapter \ref{cha:Approach}, section \ref{sec:app_pre-processing}, the step of pre-processing is an important component of every machine-learning task. Especially as it also has a significant influence on the performance of the machine-learning model. This means also, that it has a significant influence on the output and its quality, with special regard to the outcome of this work. Therefore this section describes the influence of pre-processing that has been observed during this work. 

For the task of neural audio synthesis, there exist different methods that differ mainly in the shape of their input data. While there exist methods that use the time-domain signal, a few methods including this one, use spectrograms for this task. To generate spectrograms from audio, the pre-processing stage incorporates this task. 

\subsection{Configuration of STFT}
Like discussed earlier, to obtain spectrograms, the STFT was applied which could be parameterized. This parameterization includes the size of the signal-window, on which the Fourier transform gets applied to. Throughout this work this parameter got altered, as it influences the frequency- but also time-resolution of the spectrogram. First on this parameter was chosen to be 512 which means a frequency resolution of 31,25 Hz and time 32 ms (with sampling rate of 16 kHz). Due to the unsatisfactory model performance, this value has been changed to 1024 to have a frequency resolution of 15,625 Hz and time-resolution of 64 ms. This has also the advantage of having a better frequency resolution and therefore also more features to learn from along the frequency axis. Therefore this value was chosen throughout all experiments.

\subsection{Use of single frequency vectors vs. spectrogram frames}
As it could be seen throughout the experiments, having the spectrograms, those can also be differently prepared and used for the model. First on just the single frequency vectors have been chosen to be used, as input for the model. Later on overlapping frames of spectrogram slices were utilized as model inputs. Using the first one, this has the advantage that just one frequency vector is needed as input to generate an output. Especially in terms of aiming to develop a system that performs real-time audio synthesis this would be the ideal choice. Nevertheless by comparing the results of those two approaches, the latter is proven to have a better output quality. As this also deals mainly with the model configuration, this point gets discussed in more detail later on when assessing the impact of the model configuration. 

\subsection{log-magnitude vs. log-mel}
As mentionend before, there are also other methods on how to use audio data for machine-learnig tasks. Except from using the raw time-domain signal, there also exist different types of spectrograms that can be used. First on during this work log-magnitude spectrograms have been used as the source input data for the models. Further on in the final experiments log-mel spectrograms have been used, which are based on the mel scale. By perfoming the same experiments, as with log-magnitude spectrograms, the results should bring an insight on the impact, the pre-processing or use of a different scale has. In the last chapter the error scores, but also the output spectrograms and embeddings of the models, using both types of spectrograms have been shown. With those results it already can be seen, that it has a significant impact, how the audio data is pre-processed and provided for the neural network. First of all regarding the error scores it can be said that, without taking the networks configuration into account, better scores could be reached, using the log-magnitude spectrograms. Regarding the scores for single pitch classes, different behaviour could be observed. As the scores in the log-magnitude spectrogram trained networks, remain equally among the pitch classes, the ones of the log-mel spectrogram trained networks increase with the pitch. This means that using the log-mel spectrograms, the networks have a bigger error and thus more difficulties in reconstructing higher pitched samples than lower ones. On the other side the networks with log-magnitude spectrograms, can reconstruct the input equally good, with no regard to the pitch. As this is just the performance of the network, regarding the reconstruction error, this does not automatically mean, that the listening quality is better or worse. To as



\section{Impact of post-processing}

\subsection{Energy correction}

\section{Differences between instruments}

\section{Comparison to other approaches}

\section{Limitations/difficulties}


\section{Outlook}


